{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BicycleGAN\n",
    "https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/bicyclegan/bicyclegan.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "import datetime\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "from datasets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config as opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"images/%s\" % opt.dataset_name, exist_ok=True)\n",
    "os.makedirs(\"saved_models/%s\" % opt.dataset_name, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True if torch.cuda.is_available() else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (opt.channels, opt.img_height, opt.img_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "mae_loss = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize generator, encoder and discriminators\n",
    "generator = Generator(opt.latent_dim, input_shape)\n",
    "encoder = Encoder(opt.latent_dim, input_shape)\n",
    "D_VAE = MultiDiscriminator(input_shape)\n",
    "D_LR = MultiDiscriminator(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cuda:\n",
    "    generator = generator.cuda()\n",
    "    encoder.cuda()\n",
    "    D_VAE = D_VAE.cuda()\n",
    "    D_LR = D_LR.cuda()\n",
    "    mae_loss.cuda()\n",
    "\n",
    "if opt.epoch != 0:\n",
    "    # Load pretrained models\n",
    "    generator.load_state_dict(torch.load(\"saved_models/%s/generator_%d.pth\" % (opt.dataset_name, opt.epoch)))\n",
    "    encoder.load_state_dict(torch.load(\"saved_models/%s/encoder_%d.pth\" % (opt.dataset_name, opt.epoch)))\n",
    "    D_VAE.load_state_dict(torch.load(\"saved_models/%s/D_VAE_%d.pth\" % (opt.dataset_name, opt.epoch)))\n",
    "    D_LR.load_state_dict(torch.load(\"saved_models/%s/D_LR_%d.pth\" % (opt.dataset_name, opt.epoch)))\n",
    "else:\n",
    "    # Initialize weights\n",
    "    generator.apply(weights_init_normal)\n",
    "    D_VAE.apply(weights_init_normal)\n",
    "    D_LR.apply(weights_init_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer_E = torch.optim.Adam(encoder.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "optimizer_D_VAE = torch.optim.Adam(D_VAE.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "optimizer_D_LR = torch.optim.Adam(D_LR.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    ImageDataset(\"../data/%s\" % opt.dataset_name, input_shape),\n",
    "    batch_size=opt.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=opt.n_cpu,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = DataLoader(\n",
    "    ImageDataset(\"../data/%s\" % opt.dataset_name, input_shape, mode=\"val\"),\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(batches_done):\n",
    "    \"\"\"Saves a generated sample from the validation set\"\"\"\n",
    "    generator.eval()\n",
    "    imgs = next(iter(val_dataloader))\n",
    "    img_samples = None\n",
    "    for img_A, img_B in zip(imgs[\"A\"], imgs[\"B\"]):\n",
    "        # Repeat input image by number of desired columns\n",
    "        real_A = img_A.view(1, *img_A.shape).repeat(opt.latent_dim, 1, 1, 1)\n",
    "        real_A = Variable(real_A.type(Tensor))\n",
    "        # Sample latent representations\n",
    "        sampled_z = Variable(Tensor(np.random.normal(0, 1, (opt.latent_dim, opt.latent_dim))))\n",
    "        # Generate samples\n",
    "        fake_B = generator(real_A, sampled_z)\n",
    "        # Concatenate samples horisontally\n",
    "        fake_B = torch.cat([x for x in fake_B.data.cpu()], -1)\n",
    "        img_sample = torch.cat((img_A, fake_B), -1)\n",
    "        img_sample = img_sample.view(1, *img_sample.shape)\n",
    "        # Concatenate with previous samples vertically\n",
    "        img_samples = img_sample if img_samples is None else torch.cat((img_samples, img_sample), -2)\n",
    "    save_image(img_samples, \"images/%s/%s.png\" % (opt.dataset_name, batches_done), nrow=8, normalize=True)\n",
    "    generator.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparameterization(mu, logvar):\n",
    "    std = torch.exp(logvar / 2)\n",
    "    sampled_z = Variable(Tensor(np.random.normal(0, 1, (mu.size(0), opt.latent_dim))))\n",
    "    z = sampled_z * std + mu\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 199/200] [Batch 65/66] [Batches done: 13199] [D VAE_loss: 0.916875, LR_loss: 0.359734] [G loss: 7.531536, pixel: 0.248260, kl: 13.689451, latent: 0.338497] ETA: 0:00:00.22223450068179\n",
      "Saved model at: epoch: 199, batch: 65, total batch's: 13199\n"
     ]
    }
   ],
   "source": [
    "# ----------\n",
    "#  Training\n",
    "# ----------\n",
    "# Adversarial loss\n",
    "valid = 1\n",
    "fake = 0\n",
    "\n",
    "prev_time = time.time()\n",
    "for epoch in range(opt.epoch, opt.n_epochs):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "\n",
    "        # Set model input\n",
    "        real_A = Variable(batch[\"A\"].type(Tensor))\n",
    "        real_B = Variable(batch[\"B\"].type(Tensor))\n",
    "\n",
    "        # -------------------------------\n",
    "        #  Train Generator and Encoder\n",
    "        # -------------------------------\n",
    "\n",
    "        optimizer_E.zero_grad()\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # ----------\n",
    "        # cVAE-GAN\n",
    "        # ----------\n",
    "\n",
    "        # Produce output using encoding of B (cVAE-GAN)\n",
    "        mu, logvar = encoder(real_B)\n",
    "        encoded_z = reparameterization(mu, logvar)\n",
    "        fake_B = generator(real_A, encoded_z)\n",
    "\n",
    "        # Pixelwise loss of translated image by VAE\n",
    "        loss_pixel = mae_loss(fake_B, real_B)\n",
    "        # Kullback-Leibler divergence of encoded B\n",
    "        loss_kl = 0.5 * torch.sum(torch.exp(logvar) + mu ** 2 - logvar - 1)\n",
    "        # Adversarial loss\n",
    "        loss_VAE_GAN = D_VAE.compute_loss(fake_B, valid)\n",
    "\n",
    "        # ---------\n",
    "        # cLR-GAN\n",
    "        # ---------\n",
    "\n",
    "        # Produce output using sampled z (cLR-GAN)\n",
    "        sampled_z = Variable(Tensor(np.random.normal(0, 1, (real_A.size(0), opt.latent_dim))))\n",
    "        _fake_B = generator(real_A, sampled_z)\n",
    "        # cLR Loss: Adversarial loss\n",
    "        loss_LR_GAN = D_LR.compute_loss(_fake_B, valid)\n",
    "\n",
    "        # ----------------------------------\n",
    "        # Total Loss (Generator + Encoder)\n",
    "        # ----------------------------------\n",
    "\n",
    "        loss_GE = loss_VAE_GAN + loss_LR_GAN + opt.lambda_pixel * loss_pixel + opt.lambda_kl * loss_kl\n",
    "\n",
    "        loss_GE.backward(retain_graph=True)\n",
    "        optimizer_E.step()\n",
    "\n",
    "        # ---------------------\n",
    "        # Generator Only Loss\n",
    "        # ---------------------\n",
    "\n",
    "        # Latent L1 loss\n",
    "        _mu, _ = encoder(_fake_B)\n",
    "        loss_latent = opt.lambda_latent * mae_loss(_mu, sampled_z)\n",
    "\n",
    "        loss_latent.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ----------------------------------\n",
    "        #  Train Discriminator (cVAE-GAN)\n",
    "        # ----------------------------------\n",
    "\n",
    "        optimizer_D_VAE.zero_grad()\n",
    "\n",
    "        loss_D_VAE = D_VAE.compute_loss(real_B, valid) + D_VAE.compute_loss(fake_B.detach(), fake)\n",
    "\n",
    "        loss_D_VAE.backward()\n",
    "        optimizer_D_VAE.step()\n",
    "\n",
    "        # ---------------------------------\n",
    "        #  Train Discriminator (cLR-GAN)\n",
    "        # ---------------------------------\n",
    "\n",
    "        optimizer_D_LR.zero_grad()\n",
    "\n",
    "        loss_D_LR = D_LR.compute_loss(real_B, valid) + D_LR.compute_loss(_fake_B.detach(), fake)\n",
    "\n",
    "        loss_D_LR.backward()\n",
    "        optimizer_D_LR.step()\n",
    "\n",
    "        # --------------\n",
    "        #  Log Progress\n",
    "        # --------------\n",
    "\n",
    "        # Determine approximate time left\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        batches_left = opt.n_epochs * len(dataloader) - batches_done\n",
    "        time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n",
    "        prev_time = time.time()\n",
    "\n",
    "        # Print log\n",
    "        sys.stdout.write(\n",
    "            \"\\r[Epoch %d/%d] [Batch %d/%d] [Batches done: %d] [D VAE_loss: %f, LR_loss: %f] [G loss: %f, pixel: %f, kl: %f, latent: %f] ETA: %s\"\n",
    "            % (\n",
    "                epoch,\n",
    "                opt.n_epochs,\n",
    "                i,\n",
    "                len(dataloader),\n",
    "                batches_done,\n",
    "                loss_D_VAE.item(),\n",
    "                loss_D_LR.item(),\n",
    "                loss_GE.item(),\n",
    "                loss_pixel.item(),\n",
    "                loss_kl.item(),\n",
    "                loss_latent.item(),\n",
    "                time_left,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        if batches_done % opt.sample_interval == 0:\n",
    "            sample_images(batches_done)\n",
    "\n",
    "    if opt.checkpoint_interval != -1 and (epoch+1) % opt.checkpoint_interval == 0 and i == len(dataloader)-1:\n",
    "        # Save model checkpoints\n",
    "        torch.save(generator.state_dict(), \"saved_models/%s/generator_%d.pth\" % (opt.dataset_name, epoch+1))\n",
    "        torch.save(encoder.state_dict(), \"saved_models/%s/encoder_%d.pth\" % (opt.dataset_name, epoch+1))\n",
    "        torch.save(D_VAE.state_dict(), \"saved_models/%s/D_VAE_%d.pth\" % (opt.dataset_name, epoch+1))\n",
    "        torch.save(D_LR.state_dict(), \"saved_models/%s/D_LR_%d.pth\" % (opt.dataset_name, epoch+1))\n",
    "        print(\"\\nSaved model at: \" + \"epoch: \" + str(epoch) + \", batch: \" + str(i) + \", total batch's: \" + str(batches_done))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test på egengenererte masker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from testdata import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataloader = DataLoader(\n",
    "#     TestImageDataset(\"../data/%s\" % opt.dataset_name, input_shape),\n",
    "#     batch_size=8,\n",
    "#     shuffle=True,\n",
    "#     num_workers=1,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_sample_images():\n",
    "#     \"\"\"Saves a generated sample from the validation set\"\"\"\n",
    "    \n",
    "#     localtime = time.localtime(time.time())\n",
    "#     the_time = str(localtime.tm_mday) + '.' + str(localtime.tm_mon) + '.' + str(localtime.tm_year) + '-' + str(localtime.tm_hour) + ':' + str(localtime.tm_min)\n",
    "    \n",
    "#     generator.eval()\n",
    "#     imgs = next(iter(test_dataloader))\n",
    "#     img_samples = None\n",
    "#     for img_A in imgs[\"A\"]:\n",
    "#         # Repeat input image by number of desired columns\n",
    "#         real_A = img_A.view(1, *img_A.shape).repeat(opt.latent_dim, 1, 1, 1)\n",
    "#         real_A = Variable(real_A.type(Tensor))\n",
    "#         # Sample latent representations\n",
    "#         sampled_z = Variable(Tensor(np.random.normal(0, 1, (opt.latent_dim, opt.latent_dim))))\n",
    "#         # Generate samples\n",
    "#         fake_B = generator(real_A, sampled_z)\n",
    "#         # Concatenate samples horisontally\n",
    "#         fake_B = torch.cat([x for x in fake_B.data.cpu()], -1)\n",
    "#         img_sample = torch.cat((img_A, fake_B), -1)\n",
    "#         img_sample = img_sample.view(1, *img_sample.shape)\n",
    "#         # Concatenate with previous samples vertically\n",
    "#         img_samples = img_sample if img_samples is None else torch.cat((img_samples, img_sample), -2)\n",
    "#     save_image(img_samples, \"test_images/%s/%s.jpg\" % (opt.dataset_name, the_time), nrow=8, normalize=True)\n",
    "#     #generator.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time;\n",
    "\n",
    "# localtime = time.localtime(time.time())\n",
    "# print (\"Local current time :\", localtime.tm_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (\"Local current time :\", localtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# localtime = time.localtime(time.time())\n",
    "# the_time = str(localtime.tm_mday) + '.' + str(localtime.tm_mon) + '.' + str(localtime.tm_year) + '-' + str(localtime.tm_hour) + ':' + str(localtime.tm_min)\n",
    "# the_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_sample_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_sample_images2():\n",
    "#     \"\"\"Saves a generated sample from the validation set\"\"\"\n",
    "    \n",
    "#     localtime = time.localtime(time.time())\n",
    "#     the_time = str(localtime.tm_mday) + '.' + str(localtime.tm_mon) + '.' + str(localtime.tm_year) + '-' + str(localtime.tm_hour) + ':' + str(localtime.tm_min)\n",
    "    \n",
    "#     imgs = next(iter(test_dataloader))\n",
    "#     img_samples = None\n",
    "#     for img_A in imgs[\"A\"]:\n",
    "#         # Repeat input image by number of desired columns\n",
    "#         real_A = img_A.view(1, *img_A.shape).repeat(opt.latent_dim, 1, 1, 1)\n",
    "#         real_A = Variable(real_A.type(Tensor))\n",
    "#         # Sample latent representations\n",
    "#         sampled_z = Variable(Tensor(np.random.normal(0, 1, (opt.latent_dim, opt.latent_dim))))\n",
    "#         # Generate samples\n",
    "#         fake_B = generator(real_A, sampled_z)\n",
    "#         # Concatenate samples horisontally\n",
    "#         fake_B = torch.cat([x for x in fake_B.data.cpu()], -1)\n",
    "#         img_sample = torch.cat((img_A, fake_B), -1)\n",
    "#         img_sample = img_sample.view(1, *img_sample.shape)\n",
    "#         # Concatenate with previous samples vertically\n",
    "#         img_samples = img_sample if img_samples is None else torch.cat((img_samples, img_sample), -2)\n",
    "#     save_image(img_samples, \"test_images/%s/%s.jpg\" % (opt.dataset_name, the_time), nrow=8, normalize=True)\n",
    "#     #return img_samples\n",
    "#     return img_A, fake_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kano_A, kano_B = test_sample_images2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kano_B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_sample = torch.cat((kano_A, kano_B), -1)\n",
    "# print(*img_sample.shape)\n",
    "# img_sample = img_sample.view(1, *img_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(kano_B.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kano_c = kano_B[0:3, 0:128, 640:768]#:896]#896:1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kano_c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(kano_c.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fastai2)",
   "language": "python",
   "name": "fastai2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
